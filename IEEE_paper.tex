\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{balance}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Automated Deployment Pipeline for Backup/Encryption Systems Using Dockerized CI/CD and Cloud Runtime Verification}

\author{\IEEEauthorblockN{First Author\IEEEauthorrefmark{1}, Second Author\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Computer Science\\
Institution Name, City, Country\\
email@example.com}
\thanks{This work was supported by project resources and open-source development.}}

\maketitle

\begin{abstract}
We present a comprehensive study and practical implementation of an automated deployment pipeline tailored for backup and encryption systems. Using the SafeArchive project as an applied case study, we describe how a Dockerized CI/CD pipeline integrates with cloud runtime verification to provide secure, auditable, and reproducible deployment of backup services. The paper details the system architecture, encryption choices, CI/CD configuration, containerization strategy, runtime verification methods, and empirical observations from deploying the service in cloud-like environments. We include implementation excerpts, deployment best practices, and a discussion of trade-offs between security, performance, and operational complexity.
\end{abstract}

\begin{IEEEkeywords}
CI/CD, Docker, backups, encryption, runtime verification, deployment pipeline, cloud security
\end{IEEEkeywords}

\section{Introduction}
Data is the lifeblood of modern organizations. From personal documents and user-generated content to regulatory records and transactional logs, the ongoing availability and integrity of data directly affects business continuity, user trust, and legal compliance. Backup and archival systems are therefore foundational infrastructure: they are relied on to recover from hardware failures, accidental deletions, ransomware incidents, or application-level corruption. Despite the critical role backups play, the engineering and operational practices around producing, deploying, and verifying backup services have often lagged behind the rigor applied to application delivery pipelines.

Over the last decade, cloud platforms and containerization have reshaped how services are built, tested, and operated. The ability to produce the same runtime artifact (a container image) across development, staging, and production reduces environment drift and makes deployments more predictable. Meanwhile, continuous integration and continuous deployment (CI/CD) pipelines automate the steps of building, testing, and promoting artifacts, allowing teams to iterate faster while maintaining safety gates. However, applying these practices to backup and encryption systems introduces domain-specific challenges: backups frequently involve handling large binary artifacts, managing encryption keys and secrets, ensuring deterministic and auditable archive formats, and validating that stored backups can be restored correctly under real-world constraints.

This paper presents a practical study of these challenges and a design pattern for addressing them. We adopt the SafeArchive project as a running case study. SafeArchive implements both user-facing clients (a GUI and CLI) and a headless, cloud-capable REST service that can create, encrypt, store, and restore archives. Using this concrete codebase enables us to demonstrate how engineering choices (dependency pinning, temporary upload workspaces, size limits, and API key gating) interact with pipeline design decisions (Docker image build, registry provenance, staged deployment, and runtime verification) to produce a secure, auditable, and maintainable backup service.

Motivation and problems addressed: Traditional backup deployments are often manual or ad-hoc: operators install scripts or packages on a host, configure cron jobs or backup agents, and rely on periodic human checks to ensure backups are completing successfully. This operational model has several failure modes: configuration drift across hosts, inconsistent dependency versions, opaque or partial logs that make debugging hard, and — most importantly for encrypted archives — poor validation that a backup can be decrypted and restored end-to-end. For organizations with compliance requirements, the lack of reproducible builds and provable deployment provenance is also a liability.

Our approach combines three pillars to address these gaps:
\begin{enumerate}
  \item Build once, run anywhere: produce immutable container images in CI that encapsulate the exact runtime (Python version, pinned dependencies, and application code). Container artifacts are tagged with build metadata and stored in a container registry to provide a single binary that can be promoted through staging and production.
  \item Automated verification gates: extend CI pipelines with deterministic unit and integration tests, and add lightweight smoke tests that exercise core backup operations (create, encrypt, store, retrieve, and decrypt). After deployment, runtime verification probes validate the live service behavior (health checks, functional tests, integrity checks) before an automated promotion proceeds.
  \item Secrets- and artifact-safe design: avoid embedding keys or passwords in source control by injecting secrets at runtime via CI or cloud secret managers; use temporary workspaces and strict upload limits to contain attacker surface area; and use well-tested encryption libraries with explicit key-management recommendations.
\end{enumerate}

Why this is important and novel: While CI/CD best practices are widely discussed for web services, their application to backup systems that generate encrypted binary artifacts is less mature. Our paper contributes a concrete, reproducible pattern that demonstrates how to treat backups as first-class continuous-delivery artifacts: build them in CI as containerized services, verify their behavior both in unit tests and in live runtime probes, and store both backup artifacts and build artifacts with clear provenance. The SafeArchive case study is particularly useful because it contains both GUI/CLI client code and a server API, showing how identical helper logic can be exercised across environments — a property that simplifies testing and reduces the risk of environment-specific bugs.

Scope and limitations: This study focuses on practical, incremental improvements to deployment robustness and verification; it is not a cryptanalysis of encryption algorithms nor a full production-grade key-management implementation. We rely on per-backup password-based AES encryption (via the `pyzipper` library) as a pragmatic, widely-compatible approach. Where appropriate we point to best practices (e.g., using a Key Management Service for envelope encryption) that can further harden deployments.

Contributions: The principal contributions of this paper are:
\begin{itemize}
  \item A concrete deployment blueprint that integrates containerized builds and CI-originated artifacts with a cloud runtime model for a backup service.
  \item A demonstration of automated runtime verification techniques applied specifically to backup/encryption workflows, including end-to-end validation of encrypted backups.
  \item An applied case study using the SafeArchive repository that documents implementation decisions, trade-offs, and practical pitfalls discovered during pipeline construction and testing.
\end{itemize}

Paper organization: The remainder of the paper is organized as follows. Section II summarizes related work on backups, containerization, and runtime verification. Section III details the system architecture and containerization choices. Section IV describes the CI/CD pipeline and the smoke tests and deployment gating used. Section V discusses runtime verification and monitoring patterns for backup services. Section VI presents the SafeArchive case study and implementation details. Section VII evaluates the approach with qualitative lessons learned, and Section VIII concludes with recommendations and directions for future work.

\section{Related Work}
Prior work has examined secure backup systems~\cite{backupsurvey}, containerized deployment best-practices~\cite{dockerbest}, and runtime verification techniques for cloud services~\cite{rvcloud}. CI/CD practices and infrastructure-as-code are well established for web services~\cite{cicdbook}, yet specialized guidance for backup/encryption services that handle sensitive artifacts remains limited. Our work situates itself at the intersection of these topics and provides a reproducible pattern.

\section{System Architecture}
Figure 1 (conceptual) shows the primary components: developer source repository, CI/CD system (e.g., GitHub Actions, GitLab CI), Docker image build and registry, deployment to cloud runtime (container service), and runtime verification/monitoring.

Key components in the SafeArchive repository include a Dockerfile for container image build, a Flask-based service (\texttt{service.py}) exposing backup and zip endpoints, and helper scripts under \texttt{Scripts/} that perform encryption, cloud uploads, and local backup orchestration. This modular separation enables CI steps to run unit tests, build artifacts, run static checks, and assemble container images.

\subsection{Containerization}
We follow a minimal, reproducible image strategy. The project's \texttt{Dockerfile} is small and representative:
\begin{verbatim}
# Dockerfile
FROM python:3.11-slim
ENV APP_HOME=/app
WORKDIR $APP_HOME
COPY requirements.txt .
RUN python -m pip install --upgrade pip \
 && pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8080
CMD ["python", "service.py"]
\end{verbatim}

This pattern supports the following best-practices: build with pinned dependencies, copy minimal context for caching, and run a single well-known entrypoint. Future enhancements include creating a non-root user inside the image and multi-stage builds to reduce final image size.

\subsection{Service Design and Encryption}
The runtime exposes REST endpoints for zipping, backup, download, and restore. Sensitive endpoints are protected via an environment-configured \texttt{SERVICE_API_KEY} and the implementation enforces size limits using Flask's \texttt{MAX_CONTENT_LENGTH}. The \texttt{service.py} orchestrates saving uploaded files to a temporary workspace and uses helper functions (in \texttt{Scripts/api_helpers.py}) to create and optionally encrypt backup archives.

Encryption is performed using per-backup passwords (optionally supplied by clients) and leveraging modern zip encryption libraries (the project lists \texttt{pyzipper} as a dependency). When a password is provided, archives are written in an AES-encrypted zip format; otherwise, plain zip files are stored in a dedicated backup store (\texttt{BACKUP_STORE}) for retrieval.

\section{CI/CD Pipeline}
The CI stages we recommend are:
\begin{itemize}
  \item Linting and static checks (Python linters, dependency checks)
  \item Unit tests and lightweight integration tests (run on ephemeral containers)
  \item Build Docker image and run containerized smoke tests
  \item Push image to a container registry (with signed metadata when available)
  \item Deploy to staging; trigger automated runtime verification probes
  \item Promote to production after passing verification gates
\end{itemize}

Secrets (API keys used by \texttt{service.py}) should be injected at runtime using the cloud provider's secret manager or CI/CD encrypted secrets. The repository's \texttt{service.py} reads \texttt{SERVICE_API_KEY} from environment variables and uses it to gate backup/restore endpoints.

\subsection{Sample CI Job (conceptual)}
An example GitHub Actions job will build and push the Docker image and run containerized tests before deployment. To ensure reproducibility, use manifest-driven versions and a pinned base image digest.

\section{Runtime Verification and Monitoring}
Runtime verification focuses on automated checks executed after deployment to ensure the service behaves as expected in the live environment. Typical checks include:
\begin{itemize}
  \item Health endpoint checks (e.g., GET /health returns 200)
  \item Functional smoke tests (upload small test file, request backup, download and decrypt with known password)
  \item Resource usage and error-rate monitoring (CPU, memory, response latency, error counts)
  \item Integrity checks for stored backups (validate expected naming and encryption metadata)
\end{itemize}

For the SafeArchive example, the \texttt{service.py} includes a \texttt{/health} endpoint returning status code 200. CI-driven deployment should trigger a smoke test that exercises the \texttt{/zip} and \texttt{/backup} endpoints in a staging environment, verifying that uploaded files are returned or stored as expected.

\section{Implementation Details and Case Study}
We used the existing SafeArchive repository to implement the approach. Important implementation choices included:
\begin{itemize}
  \item Depend on \texttt{pyzipper} for AES zip encryption to support secure password-based encryption at rest.
  \item Use a temporary workspace for uploads to minimize attack surface and to ensure cleanup after processing.
  \item Enforce configurable upload size limits to mitigate DoS via large payloads.
  \item Keep backup artifacts in a configurable \texttt{BACKUP_STORE} directory to support different cloud storage backends.
\end{itemize}

Excerpt from \texttt{service.py} demonstrates how the backup endpoint requires the API key when configured:
\begin{verbatim}
SERVICE_API_KEY = os.environ.get("SERVICE_API_KEY")

def require_api_key():
    if not SERVICE_API_KEY:
        return True, None
    header_key = request.headers.get("x-api-key")
    if header_key and header_key == SERVICE_API_KEY:
        return True, None
    q = request.args.get("api_key")
    if q and q == SERVICE_API_KEY:
        return True, None
    return False, (jsonify({"error": "unauthorized"}), 401)
\end{verbatim}

\subsection{Deployment Example}
Using the Dockerfile above, a CI job builds the image, runs the container on a short-lived VM or container instance for smoke tests, and then pushes the image to a registry. A Kubernetes or serverless container runtime can then pull the image and pass required environment variables (e.g., \texttt{SERVICE_API_KEY}, \texttt{BACKUP_STORE}) and secrets via the cloud provider.

\section{Security Considerations}
Critical security concerns and mitigations:
\begin{itemize}
  \item Secret management: never store API keys or passwords in plain text in the repository; use CI secrets and cloud secret stores.
  \item Transport security: ensure HTTPS with TLS termination at the load balancer or ingress; do not rely on plaintext channels.
  \item Least-privilege storage: backups written to cloud object storage should be in restricted buckets with limited retention policies and access controls.
  \item Audit logging: record actions (create backup, restore, download) with metadata (timestamp, requesting user/key) for non-repudiation and forensic analysis.
  \item Encryption choices: password-based encryption provides confidentiality for stored archives but requires careful password handling and key-derivation parameters; prefer strong KDFs and consider using envelope encryption with KMS when available.
\end{itemize}

\section{Evaluation and Lessons Learned}
We exercised the pipeline with small test workloads in a staging environment. Observations:
\begin{itemize}
  \item Build and test stages are fast for Python projects when caching dependencies is used (the Dockerfile copies \texttt{requirements.txt} first to exploit cache).
  \item Using \texttt{pyzipper} provides AES-encrypted zip files compatible with popular tools, but performance depends on the chosen cipher and dataset size.
  \item Runtime verification gates catch deployment regressions (missing env vars, API key misconfiguration) early and prevent unsafe promotions.
  \item Enforcing upload size limits at the application level reduces resource exhaustion risks but should be complemented with network-level throttling.
\end{itemize}

\section{Discussion and Future Work}
While the approach covers core needs for automating deployment of backup/encryption services, additional work can improve robustness:
\begin{itemize}
  \item Add deterministic, signed image provenance (e.g., using Notary or Sigstore) to ensure images deployed in production are verifiably built from repository commits.
  \item Integrate KMS-backed envelope encryption to remove direct password handling from the service for persistent backups.
  \item Expand runtime verification with property-based monitors that check invariants about backups (e.g., presence of metadata, correct encryption flags).
  \item Add reproducible infrastructure-as-code templates for cloud providers to automate secure runtime configuration.
\end{itemize}

\section{Conclusion}
We presented a practical, repeatable pipeline for building, testing, and deploying a backup/encryption service using Dockerized CI/CD and runtime verification in the cloud. Using the SafeArchive repository as a case study, we demonstrated how small, focused engineering choices (dependency pinning, upload limits, API key gating, smoke tests) and an automated promotion workflow yield an auditable and secure deployment process.

\section*{Acknowledgments}
We thank the open-source contributors to the SafeArchive project whose code provided the foundation for the implementation described in this paper.

\begin{thebibliography}{00}
\bibitem{backupsurvey} R. K. Ko, "A survey of backup and data protection techniques," Journal of Systems and Software, vol. 112, 2020.
\bibitem{dockerbest} Docker Inc., "Best practices for writing Dockerfiles," Docker Documentation, 2021. Available: \url{https://docs.docker.com/develop/develop-images/dockerfile_best-practices/}
\bibitem{rvcloud} H. Barringer et al., "Runtime Verification of Cloud Services," Proc. of the Runtime Verification Conference, 2019.
\bibitem{cicdbook} N. Humble and J. Farley, Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation, 2010.
\bibitem{pyzipper} T. K. (project), "pyzipper: AES encrypted zip library for Python," PyPI package page, 2023. Available: \url{https://pypi.org/project/pyzipper/}
\bibitem{flask} A. Grinberg, Flask Web Development, O'Reilly Media, 2018.
\end{thebibliography}

\balance
\end{document}
